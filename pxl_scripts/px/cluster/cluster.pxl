# Copyright (c) Pixie Labs, Inc.
# Licensed under the Apache License, Version 2.0 (the "License")

''' Cluster Overview

This view lists the namespaces and the nodes that are available on the current cluster.

'''

import px


bytes_per_mb = 1024.0 * 1024.0
# Window size to use on time_ column for bucketing.
window_s = 10
# Flag to filter out requests that come from an unresolvable IP.
filter_unresolved_inbound = True
# Flag to filter out health checks from the data.
filter_health_checks = True
# Flag to filter out ready checks from the data.
filter_ready_checks = True


def nodes_for_cluster(start_time: str):
    ''' Gets a list of nodes in the current cluster since `start_time`.

    Args:
    @start: Start time of the data to examine.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.node = df.ctx['node_name']
    df.pod = df.ctx['pod_name']
    agg = df.groupby(['node', 'pod']).agg()
    nodes = agg.groupby('node').agg(pod_count=('pod', px.count))
    process_stats = process_stats_by_entity(start_time, 'node')
    output = process_stats.merge(nodes, how='inner', left_on='node', right_on='node',
                                 suffixes=['', '_x'])
    return output[['node', 'cpu_pct', 'pod_count']]


def process_stats_by_entity(start_time: str, entity: str):
    ''' Gets the windowed process stats (CPU, memory, etc) per node or pod.

    Args:
    @start: Starting time of the data to examine.
    @entity: Either pod or node_name.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df[entity] = df.ctx[entity]
    df.timestamp = px.bin(df.time_, px.seconds(window_s))

    # Convert bytes to MB.
    df.vsize_mb = df.vsize_bytes / bytes_per_mb
    df.rss_mb = df.rss_bytes / bytes_per_mb
    df.read_mb = df.read_bytes / bytes_per_mb
    df.write_mb = df.write_bytes / bytes_per_mb
    df.rchar_mb = df.rchar_bytes / bytes_per_mb
    df.wchar_mb = df.wchar_bytes / bytes_per_mb

    # Convert nanoseconds to milliseconds.
    df.cpu_utime_ms = df.cpu_utime_ns / 1.0E6
    df.cpu_ktime_ms = df.cpu_ktime_ns / 1.0E6

    # First calculate CPU usage by process (UPID) in each k8s_object
    # over all windows.
    df = df.groupby([entity, 'upid', 'timestamp']).agg(
        rss_mb=('rss_mb', px.mean),
        vsize_mb=('vsize_mb', px.mean),
        # The fields below are counters, so we take the min and the max to subtract them.
        cpu_utime_ms_max=('cpu_utime_ms', px.max),
        cpu_utime_ms_min=('cpu_utime_ms', px.min),
        cpu_ktime_ms_max=('cpu_ktime_ms', px.max),
        cpu_ktime_ms_min=('cpu_ktime_ms', px.min),
        read_mb_max=('read_mb', px.max),
        read_mb_min=('read_mb', px.min),
        write_mb_max=('write_mb', px.max),
        write_mb_min=('write_mb', px.min),
        rchar_mb_max=('rchar_mb', px.max),
        rchar_mb_min=('rchar_mb', px.min),
        wchar_mb_max=('wchar_mb', px.max),
        wchar_mb_min=('wchar_mb', px.min),
    )

    # Next calculate cpu usage and memory stats per window.
    df.cpu_utime_ms = df.cpu_utime_ms_max - df.cpu_utime_ms_min
    df.cpu_ktime_ms = df.cpu_ktime_ms_max - df.cpu_ktime_ms_min
    df.read_mb = df.read_mb_max - df.read_mb_min
    df.write_mb = df.write_mb_max - df.write_mb_min
    df.rchar_mb = df.rchar_mb_max - df.rchar_mb_min
    df.wchar_mb = df.wchar_mb_max - df.wchar_mb_min

    # Sum by UPID.
    df = df.groupby([entity, 'timestamp']).agg(
        cpu_ktime_ms=('cpu_ktime_ms', px.sum),
        cpu_utime_ms=('cpu_utime_ms', px.sum),
        read_mb=('read_mb', px.sum),
        write_mb=('write_mb', px.sum),
        rchar_mb=('rchar_mb', px.sum),
        wchar_mb=('wchar_mb', px.sum),
        rss_mb=('rss_mb', px.sum),
        vsize_mb=('vsize_mb', px.sum),
    )

    window_size = window_s * 1.0
    df.read_mb_per_s = df.read_mb / window_size
    df.write_mb_per_s = df.write_mb / window_size
    df.rchar_mb_per_s = df.rchar_mb / window_size
    df.wchar_mb_per_s = df.wchar_mb / window_size

    # Now take the mean value over the various timestamps.
    df = df.groupby(entity).agg(
        cpu_ktime_ms=('cpu_ktime_ms', px.mean),
        cpu_utime_ms=('cpu_utime_ms', px.mean),
        read_mb_per_s=('read_mb_per_s', px.mean),
        write_mb_per_s=('write_mb_per_s', px.mean),
        rchar_mb_per_s=('rchar_mb_per_s', px.mean),
        wchar_mb_per_s=('wchar_mb_per_s', px.mean),
        avg_rss_mb=('rss_mb', px.mean),
        avg_vsize_mb=('vsize_mb', px.mean),
    )

    # Convert window_s into the same units as cpu time.
    window_size_ms = window_s * 1.0E3
    # Finally, calculate total (kernel + user time)  percentage used over window.
    df.cpu_pct = (df.cpu_ktime_ms + df.cpu_utime_ms) / window_size_ms * 100
    return df.drop(['cpu_ktime_ms', 'cpu_utime_ms'])


def pods_for_cluster(start_time: str):
    ''' A list of pods in `namespace`.

    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The name of the namespace to filter on.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.pod = df.ctx['pod_name']
    df.node = df.ctx['node_name']
    df.container = df.ctx['container_name']
    df = df.groupby(['pod', 'node', 'container']).agg()
    df = df.groupby(['pod', 'node']).agg(container_count=('container', px.count))
    df.start_time = px.pod_name_to_start_time(df.pod)
    df.status = px.pod_name_to_status(df.pod)
    process_stats = process_stats_by_entity(start_time, 'pod')
    output = process_stats.merge(df, how='inner', left_on='pod', right_on='pod',
                                 suffixes=['', '_x'])
    return output[['pod', 'cpu_pct', 'rchar_mb_per_s', 'wchar_mb_per_s', 'container_count',
                   'node', 'start_time', 'status']]


def namespaces_for_cluster(start_time: str):
    ''' Gets a overview of namespaces in the current cluster since `start_time`.

    Args:
    @start: Start time of the data to examine.
    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.service = df.ctx['service_name']
    df.pod = df.ctx['pod_name']
    df.namespace = df.ctx['namespace']
    agg = df.groupby(['service', 'pod', 'namespace']).agg()

    pod_count = agg.groupby(['namespace', 'pod']).agg()
    pod_count = pod_count.groupby('namespace').agg(pod_count=('pod', px.count))

    svc_count = agg.groupby(['namespace', 'service']).agg()
    svc_count = svc_count.groupby('namespace').agg(service_count=('service', px.count))

    pod_and_svc_count = pod_count.merge(svc_count, how='inner',
                                        left_on='namespace', right_on='namespace',
                                        suffixes=['', '_x'])

    process_stats = process_stats_by_entity(start_time, 'namespace')
    output = process_stats.merge(pod_and_svc_count, how='inner', left_on='namespace',
                                 right_on='namespace', suffixes=['', '_y'])
    return output[['namespace', 'pod_count', 'service_count', 'avg_vsize_mb', 'avg_rss_mb']]


def services_for_cluster(start_time: str):
    ''' Get an overview of the services in the current cluster.

    Args:
    @start_time: The timestamp of data to start at.

    '''
    df = px.DataFrame(table='process_stats', start_time=start_time)
    df.service = df.ctx['service']
    df = df[df.service != '']
    df.pod = df.ctx['pod']
    df = df.groupby(['service', 'pod']).agg()
    df = df.groupby('service').agg(pod_count=('pod', px.count))
    service_let = inbound_service_let_summary(start_time)
    joined = df.merge(service_let, how='inner', left_on='service', right_on='service',
                      suffixes=['', '_x'])
    return joined.drop('service_x')


def inbound_service_let_summary(start_time: str):
    ''' Compute a summary of traffic by requesting service, for requests
        on services in the current cluster..

    Args:
    @start_time: The timestamp of data to start at.

    '''
    df = inbound_service_let_helper(start_time)
    df = df[df.remote_addr != '']
    df.responder = df.service

    per_s_df = df.groupby(['timestamp', 'service']).agg(
        throughput_total=('latency_ms', px.count),
        inbound_bytes_total=('req_size', px.sum),
        outbound_bytes_total=('resp_size', px.sum)
    )

    window_size = 1.0 * window_s
    per_s_df.requests_per_s = per_s_df.throughput_total / window_size
    per_s_df.inbound_bytes_per_s = per_s_df.inbound_bytes_total / window_size
    per_s_df.outbound_bytes_per_s = per_s_df.inbound_bytes_total / window_size

    per_s_df = per_s_df.groupby('service').agg(
        requests_per_s=('requests_per_s', px.mean),
        inbound_bytes_per_s=('inbound_bytes_per_s', px.mean),
        outbound_bytes_per_s=('outbound_bytes_per_s', px.mean)
    )

    quantiles_df = df.groupby('service').agg(
        latency_ms=('latency_ms', px.quantiles)
        error_rate=('failure', px.mean),
    )

    quantiles_df.error_rate_pct = quantiles_df.error_rate * 100 / window_size

    joined = per_s_df.merge(quantiles_df, left_on='service',
                            right_on='service', how='inner',
                            suffixes=['', '_x'])
    return joined[['service', 'latency_ms', 'requests_per_s', 'error_rate_pct',
                   'inbound_bytes_per_s', 'outbound_bytes_per_s']]


def inbound_service_let_helper(start_time: str):
    ''' Compute the let as a timeseries for requests received or by services in `namespace`.

    Args:
    @start_time: The timestamp of data to start at.
    @namespace: The namespace to filter on.
    @groupby_cols: The columns to group on.

    '''
    df = px.DataFrame(table='http_events', start_time=start_time)
    df.service = df.ctx['service']
    df.pod = df.ctx['pod_name']
    df = df[df.service != '']
    df.latency_ms = df.http_resp_latency_ns / 1.0E6
    df = df[df['latency_ms'] < 10000.0]
    df.timestamp = px.bin(df.time_, px.seconds(window_s))

    df.req_size = px.length(df.http_req_body)
    df.resp_size = px.length(df.http_resp_body)
    df.failure = df.http_resp_status >= 400
    filter_out_conds = ((df.http_req_path != '/health' or not filter_health_checks) and (
        df.http_req_path != '/readyz' or not filter_ready_checks)) and (
        df['remote_addr'] != '-' or not filter_unresolved_inbound)

    df = df[filter_out_conds]
    return df


def inbound_let_service_graph(start_time: str):
    ''' Compute a summary of traffic by requesting service, for requests on services
        in the current cluster. Similar to `inbound_let_summary` but also breaks down
        by pod in addition to service.

    Args:
    @start_time: The timestamp of data to start at.
    '''
    df = inbound_service_let_helper(start_time)
    df = df.groupby(['timestamp', 'service', 'remote_addr', 'pod']).agg(
        latency_quantiles=('latency_ms', px.quantiles),
        error_rate=('failure', px.mean),
        throughput_total=('latency_ms', px.count),
        inbound_bytes_total=('req_size', px.sum),
        outbound_bytes_total=('resp_size', px.sum)
    )

    df.latency_p50 = px.pluck_float64(df.latency_quantiles, 'p50')
    df.latency_p90 = px.pluck_float64(df.latency_quantiles, 'p90')
    df.latency_p99 = px.pluck_float64(df.latency_quantiles, 'p99')

    df = df[df.remote_addr != '']
    df.responder_pod = df.pod
    df.requestor_pod_id = px.ip_to_pod_id(df.remote_addr)
    df.requestor_pod = px.pod_id_to_pod_name(df.requestor_pod_id)
    df.responder_service = df.service
    df.requestor_service = px.pod_id_to_service_name(df.requestor_pod_id)

    window_size = 1.0 * window_s
    df.requests_per_s = df.throughput_total / window_size
    df.inbound_bytes_per_s = df.inbound_bytes_total / window_size
    df.outbound_bytes_per_s = df.outbound_bytes_total / window_size
    df.error_rate_pct = df.error_rate * 100 / window_size

    return df.groupby(['responder_pod', 'requestor_pod', 'responder_service',
                       'requestor_service']).agg(
        latency_p50=('latency_p50', px.mean),
        latency_p90=('latency_p90', px.mean),
        latency_p99=('latency_p99', px.mean),
        requests_per_s=('requests_per_s', px.mean),
        error_rate_pct=('error_rate_pct', px.mean),
        inbound_bytes_per_s=('inbound_bytes_per_s', px.mean),
        outbound_bytes_per_s=('outbound_bytes_per_s', px.mean),
        throughput_total=('throughput_total', px.sum)
    )
